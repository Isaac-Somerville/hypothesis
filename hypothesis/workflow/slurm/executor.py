import dill as pickle
import hypothesis.workflow as w
import logging
import os
import shutil
import tempfile


def execute(context=None, base=None, directory='.', environment=None, store=None, cleanup=False):
    # Check if a custom context has been specified
    if context is None:
        context = w.context
    # Prune the computational graph
    context.prune()
    pickle.settings['recurse'] = True  # Handle dependencies
    # Add default Slurm attributes to the nodes
    add_default_attributes(context, base=base)
    # Set the default anaconda environment
    add_default_environment(context, environment=environment)
    # Create the generation directory
    if directory is None:
        directory = tempfile.mkdtemp()
    if not os.path.exists(directory):
        os.makedirs(directory)
    tasks_directory = directory + "/tasks"
    if not os.path.exists(tasks_directory):
        os.makedirs(tasks_directory)
    # Save the task processor
    save_processor(directory)
    # Generate the executables for the processor
    generate_executables(context, directory)
    # Generate the task files
    for node in context.nodes:
        generate_task_file(node, tasks_directory)
    # Generate the submission script
    lines = []
    lines.append("#!/usr/bin/env bash -i")
    lines.append("#")
    lines.append("# Slurm submission script, generated by Hypothesis.")
    lines.append("# github.com/montefiore-ai/hypothesis")
    lines.append("#")
    # Retrieve the tasks in BFS order
    tasks = list(context.bfs())
    task_indices = {}
    # Generate the main tasks and their dependencies
    job_id_line = "echo \""
    for task_index, task in enumerate(tasks):
        task_indices[task] = task_index
        variable = "t" + str(task_index)
        line = variable + "=$(sbatch "
        # Check if the task has dependencies
        if len(task.dependencies) > 0:
            flag = "--dependency=afterok"
            for dependency in task.dependencies:
                dependency_index = task_indices[dependency]
                flag += ":$t" + str(dependency_index)
            line += flag + " "
        line += "tasks/" + task_filename(task) + ")"
        lines.append(line)
        job_id_line += '$' + variable + "\n"
    # Create a file containing all Slurm identifiers
    job_id_line += "\" > slurm_jobs"
    lines.append(job_id_line)
    # Write the pipeline file
    pipeline_path = directory + "/pipeline.bash"
    with open(pipeline_path, "w") as f:
        for line in lines:
            f.write(line + "\n")
    # Execute the bash script
    os.system("bash " + pipeline_path)
    if store is not None:
        shutil.copyfile(directory + "/slurm_jobs", store)
    shutil.rmtree(directory + "/slurm_jobs")
    # Cleanup the generated Slurm files.
    if cleanup:
        shutil.rmtree(pipeline_path)
        shutil.rmtree(tasks_directory)


def save_processor(directory):
    processor = """
import dill as pickle
import sys

pickle.settings['recurse'] = True
with open(sys.argv[1], "rb") as f:
    function = pickle.load(f)
if len(sys.argv) > 2:
    task_index = int(sys.argv[2])
    function(task_index)
else:
    function()
"""
    with open(directory + "/processor.py", "w") as f:
        f.write(processor)


def generate_executables(context, directory):
    for node in context.nodes:
        code = pickle.dumps(node.f)
        with open(directory + "/" + node.name + ".code", "wb") as f:
            f.write(code)


def task_filename(node):
    return node.name


def add_default_environment(context, environment=None):
    if environment is not None:
        for node in context.nodes:
            node["conda"] = environment


def add_default_attributes(context, base=None):
    for node in context.nodes:
        node["--export"] = "ALL"  # Exports all environment variables,
        node["--parsable"] = ""   # Enables convenient reading of task ID.
        node["--requeue"] = ""    # Automatically requeue when something fails.
        if base is not None:
            node["--chdir"] = base


def generate_task_file(node, directory):
    lines = []
    lines.append("#!/usr/bin/env bash")
    lines.append("#")
    lines.append("#")
    lines.append("# Slurm arguments, generated by Hypothesis.")
    lines.append("# github.com/montefiore-ai/hypothesis")
    lines.append("#")
    # Add the node attributes
    for key in node.attributes:
        if key[:2] != "--":  # Skip non SBATCH arguments
            continue
        value = node[key]
        line = "#SBATCH " + key
        if len(value) > 0:
            line += "=" + value
        lines.append(line)
    # Check if the tasks is an array tasks.
    if node.tasks > 1:
        multiarray = True
        lines.append("#SBATCH --array 0-" + str(node.tasks - 1))
    else:
        multiarray = False
    # Check if a custom Anaconda environment has been specified.
    try:
        environment = node["conda"]
        lines.append("eval \"$(conda shell.bash hook)\"")
        lines.append("conda activate " + environment)
    except:
        pass
    # Execute the function
    line = "python -u processor.py " + node.name + ".code"
    if multiarray:
        line += " $SLURM_ARRAY_TASK_ID"
    lines.append(line)
    # Write the task file.
    with open(directory + "/" + task_filename(node), "w") as f:
        for line in lines:
            f.write(line + "\n")
