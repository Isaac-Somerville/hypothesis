URLS=[
"hypothesis/index.html",
"hypothesis/benchmark/index.html",
"hypothesis/benchmark/weinberg/index.html",
"hypothesis/benchmark/weinberg/simulator.html",
"hypothesis/simulation/base.html",
"hypothesis/benchmark/weinberg/util.html",
"hypothesis/benchmark/spatialsir/index.html",
"hypothesis/benchmark/spatialsir/simulator.html",
"hypothesis/benchmark/spatialsir/util.html",
"hypothesis/benchmark/tractable/index.html",
"hypothesis/benchmark/tractable/simulator.html",
"hypothesis/benchmark/tractable/util.html",
"hypothesis/benchmark/mg1/index.html",
"hypothesis/benchmark/mg1/simulator.html",
"hypothesis/benchmark/mg1/util.html",
"hypothesis/train/index.html",
"hypothesis/train/base.html",
"hypothesis/train/ratio_estimation/index.html",
"hypothesis/train/ratio_estimation/base.html",
"hypothesis/plot/index.html",
"hypothesis/auto/index.html",
"hypothesis/stat/index.html",
"hypothesis/stat/divergence.html",
"hypothesis/stat/constraint.html",
"hypothesis/bin/index.html",
"hypothesis/bin/io/index.html",
"hypothesis/bin/io/prune.html",
"hypothesis/bin/io/merge.html",
"hypothesis/bin/ratio_estimation/index.html",
"hypothesis/bin/ratio_estimation/train.html",
"hypothesis/bin/workflow/index.html",
"hypothesis/bin/workflow/processor.html",
"hypothesis/bin/workflow/cli.html",
"hypothesis/engine/index.html",
"hypothesis/engine/base.html",
"hypothesis/simulation/index.html",
"hypothesis/cli.html",
"hypothesis/workflow/index.html",
"hypothesis/workflow/local/index.html",
"hypothesis/workflow/local/executor.html",
"hypothesis/workflow/slurm/index.html",
"hypothesis/workflow/slurm/decorator.html",
"hypothesis/workflow/slurm/util.html",
"hypothesis/workflow/slurm/executor.html",
"hypothesis/workflow/base.html",
"hypothesis/workflow/decorator.html",
"hypothesis/workflow/workflow/index.html",
"hypothesis/workflow/workflow/simulate.html",
"hypothesis/workflow/util.html",
"hypothesis/workflow/graph.html",
"hypothesis/util/index.html",
"hypothesis/util/data/index.html",
"hypothesis/util/data/named.html",
"hypothesis/util/data/numpy/index.html",
"hypothesis/util/data/numpy/storage.html",
"hypothesis/util/data/numpy/dataset.html",
"hypothesis/util/data/numpy/util.html",
"hypothesis/util/base.html",
"hypothesis/nn/index.html",
"hypothesis/nn/model/index.html",
"hypothesis/nn/model/densenet/index.html",
"hypothesis/nn/model/densenet/head.html",
"hypothesis/nn/model/densenet/base.html",
"hypothesis/nn/model/densenet/util.html",
"hypothesis/nn/model/densenet/default.html",
"hypothesis/nn/model/resnet/index.html",
"hypothesis/nn/model/resnet/head.html",
"hypothesis/nn/model/resnet/base.html",
"hypothesis/nn/model/resnet/util.html",
"hypothesis/nn/model/resnet/default.html",
"hypothesis/nn/model/mlp/index.html",
"hypothesis/nn/model/mlp/base.html",
"hypothesis/nn/ratio_estimation/index.html",
"hypothesis/nn/ratio_estimation/resnet.html",
"hypothesis/nn/ratio_estimation/base.html",
"hypothesis/nn/ratio_estimation/densenet.html",
"hypothesis/nn/ratio_estimation/util.html",
"hypothesis/nn/ratio_estimation/mlp.html",
"hypothesis/nn/util.html",
"hypothesis/default.html",
"hypothesis/exception/index.html",
"hypothesis/exception/event.html"
];
INDEX=[
{
"ref":"hypothesis",
"url":0,
"doc":"Hypothesis is a python module for statistical inference and the mechanisation of science. The package contains (approximate) inference algorithms to solve inverse statistical problems. Utilities are provided for data loading, efficient simulation, visualization, fire-and-forget inference, and validation. "
},
{
"ref":"hypothesis.cpu_count",
"url":0,
"doc":"Number of available logical processor cores. The variable will be initialized when  hypothesis is loaded for the first time."
},
{
"ref":"hypothesis.workers",
"url":0,
"doc":"Default number of parallel workers in  hypothesis ."
},
{
"ref":"hypothesis.set_workers",
"url":0,
"doc":"Sets the number of default parallel hypothesis workers.",
"func":1
},
{
"ref":"hypothesis.accelerator",
"url":0,
"doc":"PyTorch device describing the accelerator backend. The variable will be initialized when  hypothesis is loaded for the first time. It will check for the availibility of a CUDA device. If a CUDA enabled device is present,  hypothesis will select the CUDA device defined in the  CUDA_VISIBLE_DEVICES environment variable. If no such device is specified, the variable will default to GPU 0."
},
{
"ref":"hypothesis.disable_gpu",
"url":0,
"doc":"Disables GPU acceleration. Hypothesis' accelerator will have been set to 'cpu'.",
"func":1
},
{
"ref":"hypothesis.enable_gpu",
"url":0,
"doc":"Tries to enable GPU acceleration. If a GPU is present, a CUDA device will be set, else it will default to 'cpu'.",
"func":1
},
{
"ref":"hypothesis.gpu_available",
"url":0,
"doc":"Checks if GPU acceleration is available.",
"func":1
},
{
"ref":"hypothesis.benchmark",
"url":1,
"doc":"Collection of benchmark problems for simulation-based inference methodologies."
},
{
"ref":"hypothesis.benchmark.weinberg",
"url":2,
"doc":"This is a simulation of high energy particle collisions  e^+e^- \\to \\mu^+ \\mu^-. The angular distributions of the particles can be used to measure the Weinberg angle in the standard model of particle physics. If you get a PhD in particle physics, you may learn how to calculate these distributions and interpret those equations to learn that an effective way to infer this parameter is to run your particle accelerator with a beam energy just above or below half the $Z$ boson mass (i.e. the optimal $\\phi$ is just above and below 45 GeV). Adapted from https: github.com/cranmer/active_sciencing/blob/master/demo_weinberg.ipynb Original implementation by Lucas Heinrich and Kyle Cranmer."
},
{
"ref":"hypothesis.benchmark.weinberg.simulator",
"url":3,
"doc":"Simulator definition of the Weinberg benchmark."
},
{
"ref":"hypothesis.benchmark.weinberg.simulator.WeinbergBenchmarkSimulator",
"url":3,
"doc":"This is a simulation of high energy particle collisions  e^+e^- \\to \\mu^+ \\mu^-. The angular distributions of the particles can be used to measure the Weinberg angle in the standard model of particle physics. If you get a PhD in particle physics, you may learn how to calculate these distributions and interpret those equations to learn that an effective way to infer this parameter is to run your particle accelerator with a beam energy just above or below half the $Z$ boson mass (i.e. the optimal $\\phi$ is just above and below 45 GeV). Adapted from https: github.com/cranmer/active_sciencing/blob/master/demo_weinberg.ipynb Original implementation by Lucas Heinrich and Kyle Cranmer.   from hypothesis.benchmark.weinberg import Prior from hypothesis.benchmark.weinberg import Simulator prior = Prior() simulator = Simulator() inputs = prior.sample 10,  Draw 10 samples from the prior outputs = simulator(inputs)  You can also batch with respect to the experimental configurations from hypothesis.benchmark.weinberg import PriorExperiment prior_experiment = PriorExperiment() beam_energies = prior_experiment.sample 10, outputs = simulator(inputs, beam_energies)  "
},
{
"ref":"hypothesis.benchmark.weinberg.simulator.WeinbergBenchmarkSimulator.MZ",
"url":3,
"doc":""
},
{
"ref":"hypothesis.benchmark.weinberg.simulator.WeinbergBenchmarkSimulator.GFNom",
"url":3,
"doc":""
},
{
"ref":"hypothesis.benchmark.weinberg.simulator.WeinbergBenchmarkSimulator.forward",
"url":3,
"doc":"Executes the forward pass of the simulation model. :param inputs: Free parameters (the Fermi constant). :param experimental_configurations: Optional experimental parameters describing the beam energy.  note This method accepts a batch of corresponding inputs and optional experimental configuration pairs.",
"func":1
},
{
"ref":"hypothesis.benchmark.weinberg.simulator.WeinbergBenchmarkSimulator.terminate",
"url":4,
"doc":"Terminates the simulator and cleans up possible contexts.  note Should be overridden by subclasses with a simulator state requiring graceful exits.",
"func":1
},
{
"ref":"hypothesis.benchmark.weinberg.util",
"url":5,
"doc":"Utilities for the Weinberg benchmark."
},
{
"ref":"hypothesis.benchmark.weinberg.util.Prior",
"url":5,
"doc":"Returns a prior  Uniform(0.25, 2.0) over the Fermi constant.",
"func":1
},
{
"ref":"hypothesis.benchmark.weinberg.util.PriorExperiment",
"url":5,
"doc":"Returns a Prior  Uniform(40.0, 50.0) over the experimental design space and represents the beam energy in GeV.",
"func":1
},
{
"ref":"hypothesis.benchmark.weinberg.util.Truth",
"url":5,
"doc":"Returns the true Fermi constant for this benchmark problem:  1.0 .",
"func":1
},
{
"ref":"hypothesis.benchmark.spatialsir",
"url":6,
"doc":"Stochastic Spatial Susceptible Infected Recovered benchmark. This problem setting is concerned with the computation of a posterior over the infection and recovery rate \\(\\vartheta\\), conditioned on an observable \\(x\\), representing a grid-world of susceptible, infected, and recovered individuals. This information is encoded in 3 individual channels. Based on these parameters, the model describes the evolution of an infection through this grid-like world. The disease spreads spatially, and is initialized with various number of initial infectious clusters, parameterized through a Poisson distribution."
},
{
"ref":"hypothesis.benchmark.spatialsir.simulator",
"url":7,
"doc":"Simulator definition of the SSIR benchmark."
},
{
"ref":"hypothesis.benchmark.spatialsir.simulator.SSIRBenchmarkSimulator",
"url":7,
"doc":"The simulation model generated a grid-world of susceptible, infected, and recovered individuals. This information is encoded in 3 individual channels. Based on these parameters, and the infection and recovery rate, the model describes the evolution of an infection through this grid-like world. The disease spreads spatially, and is initialized with various number of initial infectious clusters, parameterized through a Poisson distribution."
},
{
"ref":"hypothesis.benchmark.spatialsir.simulator.SSIRBenchmarkSimulator.simulate",
"url":7,
"doc":"",
"func":1
},
{
"ref":"hypothesis.benchmark.spatialsir.simulator.SSIRBenchmarkSimulator.forward",
"url":7,
"doc":"Defines the computation of the forward model at every call.  note Should be overridden by all subclasses.",
"func":1
},
{
"ref":"hypothesis.benchmark.spatialsir.simulator.SSIRBenchmarkSimulator.terminate",
"url":4,
"doc":"Terminates the simulator and cleans up possible contexts.  note Should be overridden by subclasses with a simulator state requiring graceful exits.",
"func":1
},
{
"ref":"hypothesis.benchmark.spatialsir.util",
"url":8,
"doc":"Utilities for the Weinberg benchmark."
},
{
"ref":"hypothesis.benchmark.spatialsir.util.Prior",
"url":8,
"doc":"Returns a uniform prior between 0 and 1 over the infection and recovery rate (encoded in this order).",
"func":1
},
{
"ref":"hypothesis.benchmark.spatialsir.util.PriorExperiment",
"url":8,
"doc":"Returns a Prior  Uniform(0.0, 10.0) over the experimental design space (measurement time). By default, the simulator will draw samples from this distribution to draw experimental configurations.",
"func":1
},
{
"ref":"hypothesis.benchmark.spatialsir.util.Truth",
"url":8,
"doc":"Returns the true infection and recovery rate of this benchmark problem:  (0.8, 0.2) .",
"func":1
},
{
"ref":"hypothesis.benchmark.tractable",
"url":9,
"doc":"Tractable benchmark"
},
{
"ref":"hypothesis.benchmark.tractable.simulator",
"url":10,
"doc":"Simulator definition of the tractable benchmark."
},
{
"ref":"hypothesis.benchmark.tractable.simulator.TractableBenchmarkSimulator",
"url":10,
"doc":"Simulation model associated with the tractable benchmark."
},
{
"ref":"hypothesis.benchmark.tractable.simulator.TractableBenchmarkSimulator.forward",
"url":10,
"doc":"Defines the computation of the forward model at every call.  note Should be overridden by all subclasses.",
"func":1
},
{
"ref":"hypothesis.benchmark.tractable.simulator.TractableBenchmarkSimulator.terminate",
"url":4,
"doc":"Terminates the simulator and cleans up possible contexts.  note Should be overridden by subclasses with a simulator state requiring graceful exits.",
"func":1
},
{
"ref":"hypothesis.benchmark.tractable.util",
"url":11,
"doc":"Utilities for the tractable benchmark."
},
{
"ref":"hypothesis.benchmark.tractable.util.Prior",
"url":11,
"doc":"",
"func":1
},
{
"ref":"hypothesis.benchmark.tractable.util.Truth",
"url":11,
"doc":"",
"func":1
},
{
"ref":"hypothesis.benchmark.tractable.util.Uniform",
"url":11,
"doc":"Generates uniformly distributed random samples from the half-open interval  [low, high) . Example >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0] >>> m.sample()  uniformly distributed in the range [0.0, 5.0) tensor([ 2.3418]) Args: low (float or Tensor): lower range (inclusive). high (float or Tensor): upper range (exclusive)."
},
{
"ref":"hypothesis.benchmark.tractable.util.Uniform.log_prob",
"url":11,
"doc":"Returns the log of the probability density/mass function evaluated at  value . Args: value (Tensor):",
"func":1
},
{
"ref":"hypothesis.benchmark.mg1",
"url":12,
"doc":"The model of this benchmark describes a queuing system of continuously arriving jobs by a single server. The time it takes to process every job is uniformly distributed in the interval \\([ theta_1,  theta_2]\\). The arrival between two consecutive jobs is exponentially distributed according to the rate \\( theta_3\\). That is, for every job \\(i\\) we have the processing time \\(p_i\\) , an arrival time \\(a_i\\) and the time \\(l_i\\) at which the job left the queue."
},
{
"ref":"hypothesis.benchmark.mg1.simulator",
"url":13,
"doc":"Simulator definition of the M/G/1 queinig model."
},
{
"ref":"hypothesis.benchmark.mg1.simulator.MG1BenchmarkSimulator",
"url":13,
"doc":"Simulation model of the M/G/1 queuing model. The model describes a queuing system of continuously arriving jobs by a single server. The time it takes to process every job is uniformly distributed in the interval \\([ theta_1,  theta_2]\\). The arrival between two consecutive jobs is exponentially distributed according to the rate \\( theta_3\\). That is, for every job \\(i\\) we have the processing time \\(p_i\\) , an arrival time \\(a_i\\) and the time \\(l_i\\) at which the job left the queue."
},
{
"ref":"hypothesis.benchmark.mg1.simulator.MG1BenchmarkSimulator.forward",
"url":13,
"doc":"Defines the computation of the forward model at every call.  note Should be overridden by all subclasses.",
"func":1
},
{
"ref":"hypothesis.benchmark.mg1.simulator.MG1BenchmarkSimulator.terminate",
"url":4,
"doc":"Terminates the simulator and cleans up possible contexts.  note Should be overridden by subclasses with a simulator state requiring graceful exits.",
"func":1
},
{
"ref":"hypothesis.benchmark.mg1.util",
"url":14,
"doc":"Utilities for the M/G/1 benchmark."
},
{
"ref":"hypothesis.benchmark.mg1.util.Prior",
"url":14,
"doc":"Returns a uniform prior between  (0, 0, 0) and  (10, 10, 1/3) .",
"func":1
},
{
"ref":"hypothesis.benchmark.mg1.util.Truth",
"url":14,
"doc":"Returns the true queuing model parameters:  (1, 5, 0.2) .",
"func":1
},
{
"ref":"hypothesis.benchmark.mg1.util.Uniform",
"url":14,
"doc":"Generates uniformly distributed random samples from the half-open interval  [low, high) . Example >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0] >>> m.sample()  uniformly distributed in the range [0.0, 5.0) tensor([ 2.3418]) Args: low (float or Tensor): lower range (inclusive). high (float or Tensor): upper range (exclusive)."
},
{
"ref":"hypothesis.benchmark.mg1.util.Uniform.log_prob",
"url":14,
"doc":"Returns the log of the probability density/mass function evaluated at  value . Args: value (Tensor):",
"func":1
},
{
"ref":"hypothesis.train",
"url":15,
"doc":"Base definition of optimization procedures and their utilities."
},
{
"ref":"hypothesis.train.base",
"url":16,
"doc":"Base definition of a neural network trainer with customizable hooks."
},
{
"ref":"hypothesis.train.base.BaseTrainer",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.base.BaseTrainer.accelerator",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.base.BaseTrainer.fit",
"url":16,
"doc":"",
"func":1
},
{
"ref":"hypothesis.train.base.BaseTrainer.train",
"url":16,
"doc":"",
"func":1
},
{
"ref":"hypothesis.train.base.BaseTrainer.validate",
"url":16,
"doc":"",
"func":1
},
{
"ref":"hypothesis.train.base.BaseTrainer.test",
"url":16,
"doc":"",
"func":1
},
{
"ref":"hypothesis.train.base.BaseTrainer.losses_test",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.base.BaseTrainer.losses_train",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.base.BaseTrainer.losses_validate",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.base.BaseTrainer.epochs",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.base.BaseTrainer.current_epoch",
"url":16,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation",
"url":17,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.conservativenesses",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.conservativeness",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.optimizer",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.estimator",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.state_dict",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.best_state_dict",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.best_estimator",
"url":18,
"doc":""
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.train",
"url":18,
"doc":"",
"func":1
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.validate",
"url":18,
"doc":"",
"func":1
},
{
"ref":"hypothesis.train.ratio_estimation.base.RatioEstimatorTrainer.test",
"url":18,
"doc":"",
"func":1
},
{
"ref":"hypothesis.plot",
"url":19,
"doc":"Base plotting utilities and definition of  matplotlib theme."
},
{
"ref":"hypothesis.auto",
"url":20,
"doc":"Automated procedures and utilities to determine quantities of interest."
},
{
"ref":"hypothesis.stat",
"url":21,
"doc":"Statistical utilities."
},
{
"ref":"hypothesis.stat.divergence",
"url":22,
"doc":"Utilities to compute divergences between densities, or samples of those densities."
},
{
"ref":"hypothesis.stat.divergence.jsd",
"url":22,
"doc":"Computes the Jensen-Shannon Divergence between samples from \\(p\\) and \\(q\\).",
"func":1
},
{
"ref":"hypothesis.stat.constraint",
"url":23,
"doc":"Utilities to obtain Bayesian credible regions or Frequentist confidence intervals."
},
{
"ref":"hypothesis.stat.constraint.likelihood_ratio_test_statistic",
"url":23,
"doc":"",
"func":1
},
{
"ref":"hypothesis.stat.constraint.confidence_level",
"url":23,
"doc":"",
"func":1
},
{
"ref":"hypothesis.stat.constraint.highest_density_level",
"url":23,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin",
"url":24,
"doc":"Predefined utility binaries including common tasks such as data-preparation, training of ratio estimators, and management of workflows defined through  hypothesis.workflow ."
},
{
"ref":"hypothesis.bin.io",
"url":25,
"doc":""
},
{
"ref":"hypothesis.bin.io.prune",
"url":26,
"doc":"A utility program to prune data files."
},
{
"ref":"hypothesis.bin.io.prune.main",
"url":26,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.prune.parse_arguments",
"url":26,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.merge",
"url":27,
"doc":"A utility program to merge data files."
},
{
"ref":"hypothesis.bin.io.merge.main",
"url":27,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.merge.procedure_numpy",
"url":27,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.merge.procedure_torch",
"url":27,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.merge.fetch_input_files",
"url":27,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.merge.select_extension_procedure",
"url":27,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.io.merge.parse_arguments",
"url":27,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation",
"url":28,
"doc":""
},
{
"ref":"hypothesis.bin.ratio_estimation.train",
"url":29,
"doc":"Utility program to train ratio estimators. This program provides a whole range of utilities to monitor and train ratio estimators in various ways. All defined through command line arguments!"
},
{
"ref":"hypothesis.bin.ratio_estimation.train.main",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.load_dataset_test",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.load_dataset_train",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.load_dataset_validate",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.load_ratio_estimator",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.load_optimizer",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.add_hooks",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.add_hooks_lr_scheduling",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.add_hooks_lr_scheduling_on_plateau",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.add_hooks_lr_scheduling_cyclic",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.ratio_estimation.train.parse_arguments",
"url":29,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow",
"url":30,
"doc":""
},
{
"ref":"hypothesis.bin.workflow.processor",
"url":31,
"doc":""
},
{
"ref":"hypothesis.bin.workflow.processor.main",
"url":31,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli",
"url":32,
"doc":"A utility program to handle Hypothesis workflows."
},
{
"ref":"hypothesis.bin.workflow.cli.main",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.store_directory",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.list_store",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.delete_workflow",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.clean_workflows",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.workflow_status",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.execute_workflow",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.goto_workflow",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.prepare_directory",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.assert_slurm_detected",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.execute_slurm",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.execute_local",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.load_default_executor",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.bin.workflow.cli.parse_arguments",
"url":32,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine",
"url":33,
"doc":"Base definitions for event-driven procedures in  hypothesis ."
},
{
"ref":"hypothesis.engine.base",
"url":34,
"doc":""
},
{
"ref":"hypothesis.engine.base.Events",
"url":34,
"doc":""
},
{
"ref":"hypothesis.engine.base.Procedure",
"url":34,
"doc":""
},
{
"ref":"hypothesis.engine.base.Procedure.register_event",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine.base.Procedure.registered_events",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine.base.Procedure.add_event_handler",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine.base.Procedure.clear_event_handler",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine.base.Procedure.clear_event_handlers",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine.base.Procedure.call_event",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.engine.base.Procedure.on",
"url":34,
"doc":"",
"func":1
},
{
"ref":"hypothesis.simulation",
"url":35,
"doc":"Submodule containing the base simulator architecture and utilities to execute efficient simulations. Every forward model requires to be wrapped in a class which inherits from  BaseSimulator ."
},
{
"ref":"hypothesis.simulation.base",
"url":4,
"doc":""
},
{
"ref":"hypothesis.simulation.base.BaseSimulator",
"url":4,
"doc":"Base simulator class. A simulator defines the implicit forward model. Example usage of a potential simulator implementation: simulator = MySimulator() inputs = prior.sample 10,  Draw 10 samples from the prior. outputs = simulator(inputs) In principle, this corresponds to sampling from the joint  \\vartheta,x\\sim p(\\vartheta)p(x\\vert\\vartheta) , where  p(x\\vert\\vartheta) is the likelihood-model implicitely defined through the simulator.  note The  inputs and  outputs variable name in most simulator denote their position with respect to the simulation model.  inputs are typically free parameters of the simulation model which sample (or produce deterministically)  outputs .  note Although it is possibly to supply a batch of inputs, it should be noted that these are currently  not parallelized."
},
{
"ref":"hypothesis.simulation.base.BaseSimulator.forward",
"url":4,
"doc":"Defines the computation of the forward model at every call.  note Should be overridden by all subclasses.",
"func":1
},
{
"ref":"hypothesis.simulation.base.BaseSimulator.terminate",
"url":4,
"doc":"Terminates the simulator and cleans up possible contexts.  note Should be overridden by subclasses with a simulator state requiring graceful exits.",
"func":1
},
{
"ref":"hypothesis.cli",
"url":36,
"doc":"Main CLI interface for  hypothesis ."
},
{
"ref":"hypothesis.cli.main",
"url":36,
"doc":"",
"func":1
},
{
"ref":"hypothesis.cli.execute_merge",
"url":36,
"doc":"",
"func":1
},
{
"ref":"hypothesis.cli.execute_prune",
"url":36,
"doc":"",
"func":1
},
{
"ref":"hypothesis.cli.execute_version",
"url":36,
"doc":"",
"func":1
},
{
"ref":"hypothesis.cli.execute_workflow",
"url":36,
"doc":"",
"func":1
},
{
"ref":"hypothesis.cli.show_help_and_exit",
"url":36,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow",
"url":37,
"doc":"Predefined workflows and utilities to build dependency graphs on your local workstation and HPC clusters. Workflows are acyclic computational graphs which essentially define a dependency graph of procedures. These graph serve as an abstraction for  executors , which execute the workflow locally or generate the required code to run the computational graph on an HPC cluster without having to write the necessarry supporting code. Everything can directly be done in Python. These workflows also tie directly with the  hypothesis workflow CLI tool."
},
{
"ref":"hypothesis.workflow.local",
"url":38,
"doc":""
},
{
"ref":"hypothesis.workflow.local.executor",
"url":39,
"doc":""
},
{
"ref":"hypothesis.workflow.local.executor.execute",
"url":39,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm",
"url":40,
"doc":""
},
{
"ref":"hypothesis.workflow.slurm.decorator",
"url":41,
"doc":""
},
{
"ref":"hypothesis.workflow.slurm.util",
"url":42,
"doc":""
},
{
"ref":"hypothesis.workflow.slurm.util.slurm_detected",
"url":42,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor",
"url":43,
"doc":""
},
{
"ref":"hypothesis.workflow.slurm.executor.execute",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.generate_executables",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.task_filename",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.executable_name",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.add_default_environment",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.add_partition",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.add_default_attributes",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.slurm.executor.generate_task_file",
"url":43,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.base",
"url":44,
"doc":""
},
{
"ref":"hypothesis.workflow.base.BaseWorkflow",
"url":44,
"doc":""
},
{
"ref":"hypothesis.workflow.base.BaseWorkflow.graph",
"url":44,
"doc":""
},
{
"ref":"hypothesis.workflow.base.BaseWorkflow.attach",
"url":44,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.base.BaseWorkflow.build",
"url":44,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.decorator",
"url":45,
"doc":""
},
{
"ref":"hypothesis.workflow.decorator.root",
"url":45,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.decorator.disable",
"url":45,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.workflow",
"url":46,
"doc":"Predefined workflows, ready for usage."
},
{
"ref":"hypothesis.workflow.workflow.simulate",
"url":47,
"doc":""
},
{
"ref":"hypothesis.workflow.workflow.simulate.SimulateBlocksWorkflow",
"url":47,
"doc":""
},
{
"ref":"hypothesis.workflow.workflow.simulate.SimulateBlocksWorkflow.n",
"url":47,
"doc":""
},
{
"ref":"hypothesis.workflow.workflow.simulate.SimulateBlocksWorkflow.blocksize",
"url":47,
"doc":""
},
{
"ref":"hypothesis.workflow.workflow.simulate.SimulateBlocksWorkflow.num_blocks",
"url":47,
"doc":""
},
{
"ref":"hypothesis.workflow.workflow.simulate.SimulateTrainTestBlocksWorkflow",
"url":47,
"doc":""
},
{
"ref":"hypothesis.workflow.util",
"url":48,
"doc":""
},
{
"ref":"hypothesis.workflow.util.add_and_get_node",
"url":48,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.util.exists",
"url":48,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.util.not_exists",
"url":48,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.util.parameterized",
"url":48,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.util.shell",
"url":48,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.util.clear",
"url":48,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.rebuild",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.add_node",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.delete_node",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.find_node",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.nodes",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.root",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.leaves",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.program",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.bfs",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.prune",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowGraph.debug",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.f",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.tasks",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.attributes",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.postconditions",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.add_postcondition",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.has_posconditions",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.postconditions_satisfied",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.name",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.disabled",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.siblings",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.parents",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.add_parent",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.remove_parent",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.add_child",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.remove_child",
"url":49,
"doc":"",
"func":1
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.dependencies",
"url":49,
"doc":""
},
{
"ref":"hypothesis.workflow.graph.WorkflowNode.children",
"url":49,
"doc":""
},
{
"ref":"hypothesis.util",
"url":50,
"doc":"Utilities used accross various subpackages of  hypothesis ."
},
{
"ref":"hypothesis.util.data",
"url":51,
"doc":""
},
{
"ref":"hypothesis.util.data.named",
"url":52,
"doc":""
},
{
"ref":"hypothesis.util.data.named.NamedDataset",
"url":52,
"doc":"An abstract class representing a :class: Dataset . All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth: __getitem__ , supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth: __len__ , which is expected to return the size of the dataset by many :class: ~torch.utils.data.Sampler implementations and the default options of :class: ~torch.utils.data.DataLoader .  note :class: ~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided."
},
{
"ref":"hypothesis.util.data.numpy",
"url":53,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.storage",
"url":54,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.storage.BaseStorage",
"url":54,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.storage.BaseStorage.close",
"url":54,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.storage.InMemoryStorage",
"url":54,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.storage.InMemoryStorage.close",
"url":54,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.storage.PersistentStorage",
"url":54,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.storage.PersistentStorage.close",
"url":54,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.dataset",
"url":55,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.dataset.Dataset",
"url":55,
"doc":"An abstract class representing a :class: Dataset . All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth: __getitem__ , supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth: __len__ , which is expected to return the size of the dataset by many :class: ~torch.utils.data.Sampler implementations and the default options of :class: ~torch.utils.data.DataLoader .  note :class: ~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided."
},
{
"ref":"hypothesis.util.data.numpy.util",
"url":56,
"doc":""
},
{
"ref":"hypothesis.util.data.numpy.util.compute_final_shape",
"url":56,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.util.merge",
"url":56,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.util.merge_in_memory",
"url":56,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.util.merge_on_disk",
"url":56,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.data.numpy.util.insert_data",
"url":56,
"doc":"",
"func":1
},
{
"ref":"hypothesis.util.base",
"url":57,
"doc":"General utilities for :mod: hypothesis ."
},
{
"ref":"hypothesis.util.base.is_iterable",
"url":57,
"doc":"Checks whether the specified item is iterable. :param item: Any possible Python instance. :rtype: bool",
"func":1
},
{
"ref":"hypothesis.util.base.is_integer",
"url":57,
"doc":"Checks whether the specified item is an integer. :param item: Any possible Python instance. :rtype: bool",
"func":1
},
{
"ref":"hypothesis.util.base.load_module",
"url":57,
"doc":"Loads the specified module (or class). :param full_modulename: The full module name of the method, class or variable to load.",
"func":1
},
{
"ref":"hypothesis.nn",
"url":58,
"doc":"Specifies neural network utilities and extensions."
},
{
"ref":"hypothesis.nn.model",
"url":59,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet",
"url":60,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head",
"url":61,
"doc":"Definition of the DenseNet head."
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseNetHead",
"url":61,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseNetHead.dump_patches",
"url":61,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseNetHead.training",
"url":61,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseNetHead.embedding_dimensionality",
"url":61,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseNetHead.forward",
"url":61,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseBlock",
"url":61,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseBlock.dump_patches",
"url":61,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseBlock.training",
"url":61,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseBlock.forward",
"url":61,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseLayer",
"url":61,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseLayer.dump_patches",
"url":61,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseLayer.training",
"url":61,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.head.DenseLayer.forward",
"url":61,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.load_configuration_121",
"url":61,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.load_configuration_161",
"url":61,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.load_configuration_169",
"url":61,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.head.load_configuration_201",
"url":61,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.base",
"url":62,
"doc":"Definition of the DenseNet architecture."
},
{
"ref":"hypothesis.nn.model.densenet.base.DenseNet",
"url":62,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.densenet.base.DenseNet.dump_patches",
"url":62,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.base.DenseNet.training",
"url":62,
"doc":""
},
{
"ref":"hypothesis.nn.model.densenet.base.DenseNet.forward",
"url":62,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.util",
"url":63,
"doc":"DenseNet-specific utilities."
},
{
"ref":"hypothesis.nn.model.densenet.util.load_modules",
"url":63,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.util.load_modules_1_dimensional",
"url":63,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.util.load_modules_2_dimensional",
"url":63,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.util.load_modules_3_dimensional",
"url":63,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.densenet.default",
"url":64,
"doc":"DenseNet defaults."
},
{
"ref":"hypothesis.nn.model.densenet.default.depth",
"url":64,
"doc":"Default DenseNet architecture (depth)."
},
{
"ref":"hypothesis.nn.model.densenet.default.batchnorm",
"url":64,
"doc":"Default batch normalization flag in Hypothesis."
},
{
"ref":"hypothesis.nn.model.densenet.default.bottleneck_factor",
"url":64,
"doc":"Default bottleneck-factor in DenseNet architectures."
},
{
"ref":"hypothesis.nn.model.densenet.default.convolution_bias",
"url":64,
"doc":"Add biases in convolutions by default in Hypothesis."
},
{
"ref":"hypothesis.nn.model.densenet.default.channels",
"url":64,
"doc":"Default number of data channels (e.g., channels in images)."
},
{
"ref":"hypothesis.nn.model.resnet",
"url":65,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head",
"url":66,
"doc":"Definition of the ResNet head."
},
{
"ref":"hypothesis.nn.model.resnet.head.ResNetHead",
"url":66,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.resnet.head.ResNetHead.dump_patches",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.ResNetHead.training",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.ResNetHead.embedding_dimensionality",
"url":66,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.ResNetHead.forward",
"url":66,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.BasicBlock",
"url":66,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.resnet.head.BasicBlock.dump_patches",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.BasicBlock.training",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.BasicBlock.EXPANSION",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.BasicBlock.forward",
"url":66,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.Bottleneck",
"url":66,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.resnet.head.Bottleneck.dump_patches",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.Bottleneck.training",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.Bottleneck.EXPANSION",
"url":66,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.head.Bottleneck.forward",
"url":66,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.load_configuration_18",
"url":66,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.load_configuration_34",
"url":66,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.load_configuration_50",
"url":66,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.load_configuration_101",
"url":66,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.head.load_configuration_152",
"url":66,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.base",
"url":67,
"doc":"Main ResNet definition."
},
{
"ref":"hypothesis.nn.model.resnet.base.ResNet",
"url":67,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.model.resnet.base.ResNet.dump_patches",
"url":67,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.base.ResNet.training",
"url":67,
"doc":""
},
{
"ref":"hypothesis.nn.model.resnet.base.ResNet.forward",
"url":67,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.util",
"url":68,
"doc":"Utilities form :mod: hypothesis.nn.model.resnet ."
},
{
"ref":"hypothesis.nn.model.resnet.util.load_modules",
"url":68,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.util.load_modules_1_dimensional",
"url":68,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.util.load_modules_2_dimensional",
"url":68,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.util.load_modules_3_dimensional",
"url":68,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.model.resnet.default",
"url":69,
"doc":"Default ResNet settings in :mod: hypothesis ."
},
{
"ref":"hypothesis.nn.model.resnet.default.depth",
"url":69,
"doc":"Default ResNet architecture (depth)."
},
{
"ref":"hypothesis.nn.model.resnet.default.batchnorm",
"url":69,
"doc":"Default batch normalization flag in Hypothesis."
},
{
"ref":"hypothesis.nn.model.resnet.default.convolution_bias",
"url":69,
"doc":"Add biases in convolutions by default in Hypothesis."
},
{
"ref":"hypothesis.nn.model.resnet.default.channels",
"url":69,
"doc":"Default number of data channels (e.g., channels in images)."
},
{
"ref":"hypothesis.nn.model.resnet.default.dilate",
"url":69,
"doc":"Default usage of dilated convolutions."
},
{
"ref":"hypothesis.nn.model.resnet.default.in_planes",
"url":69,
"doc":"Default number of input feature maps."
},
{
"ref":"hypothesis.nn.model.resnet.default.width_per_group",
"url":69,
"doc":"Default group width."
},
{
"ref":"hypothesis.nn.model.resnet.default.groups",
"url":69,
"doc":"Default number of concurrent convolutional groups."
},
{
"ref":"hypothesis.nn.model.mlp",
"url":70,
"doc":""
},
{
"ref":"hypothesis.nn.model.mlp.base",
"url":71,
"doc":"Base model of a multi-layered perceptron (MLP)."
},
{
"ref":"hypothesis.nn.model.mlp.base.MLP",
"url":71,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes a multi-layered perceptron (MLP). :param shape_xs: A tuple describing the shape of the MLP inputs. :param shape_ys: A tuple describing the shape of the MLP outputs. :param activation: An allocator which, when called, returns a :mod: torch activation. :param dropout: Dropout rate. :param transform_output: Output transformation. :rtype: :class: hypothesis.nn.model.mlp.MLP "
},
{
"ref":"hypothesis.nn.model.mlp.base.MLP.dump_patches",
"url":71,
"doc":""
},
{
"ref":"hypothesis.nn.model.mlp.base.MLP.training",
"url":71,
"doc":""
},
{
"ref":"hypothesis.nn.model.mlp.base.MLP.forward",
"url":71,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation",
"url":72,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.resnet",
"url":73,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.resnet.build_ratio_estimator",
"url":73,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator",
"url":74,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator.dump_patches",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator.training",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator.denominator",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator.random_variables",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator.forward",
"url":74,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseRatioEstimator.log_ratio",
"url":74,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble",
"url":74,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.dump_patches",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.training",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.forward",
"url":74,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.KEYWORD_REDUCE",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.REDUCE_MEAN",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.REDUCE_MEDIAN",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.parameters",
"url":74,
"doc":"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example >>> for param in model.parameters(): >>> print(type(param), param.size(  (20L,)  (20L, 1L, 5L, 5L)",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.to",
"url":74,
"doc":"Moves and/or casts the parameters and buffers. This can be called as  function to(device=None, dtype=None, non_blocking=False)  function to(dtype, non_blocking=False)  function to(tensor, non_blocking=False)  function to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  note This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor( 0.1913, -0.3420], [-0.5113, -0.2325 ) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1913, -0.3420], [-0.5113, -0.2325 , dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1914, -0.3420], [-0.5112, -0.2324 , dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1914, -0.3420], [-0.5112, -0.2324 , dtype=torch.float16)",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.reduce_as",
"url":74,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.RatioEstimatorEnsemble.log_ratio",
"url":74,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseCriterion",
"url":74,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseCriterion.dump_patches",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseCriterion.training",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseCriterion.batch_size",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseCriterion.to",
"url":74,
"doc":"Moves and/or casts the parameters and buffers. This can be called as  function to(device=None, dtype=None, non_blocking=False)  function to(dtype, non_blocking=False)  function to(tensor, non_blocking=False)  function to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  note This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor( 0.1913, -0.3420], [-0.5113, -0.2325 ) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1913, -0.3420], [-0.5113, -0.2325 , dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1914, -0.3420], [-0.5112, -0.2324 , dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1914, -0.3420], [-0.5112, -0.2324 , dtype=torch.float16)",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.BaseCriterion.forward",
"url":74,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.ConservativeCriterion",
"url":74,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.nn.ratio_estimation.base.ConservativeCriterion.dump_patches",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.ConservativeCriterion.training",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.ConservativeCriterion.forward",
"url":74,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.base.ConservativeCriterion.conservativeness",
"url":74,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.base.ConservativeCriterion.to",
"url":74,
"doc":"Moves and/or casts the parameters and buffers. This can be called as  function to(device=None, dtype=None, non_blocking=False)  function to(dtype, non_blocking=False)  function to(tensor, non_blocking=False)  function to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  note This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor( 0.1913, -0.3420], [-0.5113, -0.2325 ) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1913, -0.3420], [-0.5113, -0.2325 , dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1914, -0.3420], [-0.5112, -0.2324 , dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor( 0.1914, -0.3420], [-0.5112, -0.2324 , dtype=torch.float16)",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.densenet",
"url":75,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.densenet.build_ratio_estimator",
"url":75,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.util",
"url":76,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.util.build_ratio_estimator",
"url":76,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.ratio_estimation.mlp",
"url":77,
"doc":""
},
{
"ref":"hypothesis.nn.ratio_estimation.mlp.build_ratio_estimator",
"url":77,
"doc":"",
"func":1
},
{
"ref":"hypothesis.nn.util",
"url":78,
"doc":"Utilies for :mod: hypothesis.nn ."
},
{
"ref":"hypothesis.nn.util.allocate_output_transform",
"url":78,
"doc":"Allocates the specified output transformation for the given output shape. :param transformation:  \"normalize\" or an allocator defining the transformation. :param shape: Output shape of the transformation. :type shape: iterable of ints",
"func":1
},
{
"ref":"hypothesis.nn.util.dimensionality",
"url":78,
"doc":"Computes the total dimensionality of the specified shape. :param shape: Tuple describing the shape of the expected data. :type shape: iterable of ints :rtype: int",
"func":1
},
{
"ref":"hypothesis.default",
"url":79,
"doc":"Default settings in  hypothesis ."
},
{
"ref":"hypothesis.default.LeakyReLU",
"url":79,
"doc":"Applies the element-wise function:  math \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope}  \\min(0, x) or  math \\text{LeakyRELU}(x) = \\begin{cases} x, & \\text{ if } x \\geq 0  \\text{negative\\_slope} \\times x, & \\text{ otherwise } \\end{cases} Args: negative_slope: Controls the angle of the negative slope. Default: 1e-2 inplace: can optionally do the operation in-place. Default:  False Shape: - Input: :math: (N,  ) where  means, any number of additional dimensions - Output: :math: (N,  ) , same shape as the input  image  /scripts/activation_images/LeakyReLU.png Examples >>> m = nn.LeakyReLU(0.1) >>> input = torch.randn(2) >>> output = m(input) Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"hypothesis.default.LeakyReLU.inplace",
"url":79,
"doc":""
},
{
"ref":"hypothesis.default.LeakyReLU.negative_slope",
"url":79,
"doc":""
},
{
"ref":"hypothesis.default.LeakyReLU.forward",
"url":79,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"hypothesis.default.LeakyReLU.extra_repr",
"url":79,
"doc":"Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.",
"func":1
},
{
"ref":"hypothesis.default.batch_size",
"url":79,
"doc":"Default batch size."
},
{
"ref":"hypothesis.default.dropout",
"url":79,
"doc":"Default dropout setting."
},
{
"ref":"hypothesis.default.epochs",
"url":79,
"doc":"Default number of data epochs."
},
{
"ref":"hypothesis.default.output_transform",
"url":79,
"doc":"Default output transformation for neural networks. For 1-dimensional outputs, this is equivalent to torch.nn.Sigmoid. Otherwise, this will reduce to torch.nn.Softmax."
},
{
"ref":"hypothesis.default.trunk",
"url":79,
"doc":"Default trunk of an MLP."
},
{
"ref":"hypothesis.default.dependent_delimiter",
"url":79,
"doc":"Split character indicating the dependence between random variables."
},
{
"ref":"hypothesis.default.independent_delimiter",
"url":79,
"doc":"Split character indicating the independene between random variables."
},
{
"ref":"hypothesis.default.dataloader_workers",
"url":79,
"doc":"Default number of dataloader workers."
},
{
"ref":"hypothesis.exception",
"url":80,
"doc":"Exception definitions specific to  hypothesis ."
},
{
"ref":"hypothesis.exception.event",
"url":81,
"doc":""
},
{
"ref":"hypothesis.exception.event.NoEventRegistrationException",
"url":81,
"doc":""
},
{
"ref":"hypothesis.exception.event.NoSuchEventException",
"url":81,
"doc":""
}
]